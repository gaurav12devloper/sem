
Practical 1
AIM: Write a program to demonstrate bitwise operation
WRITE UP:
In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral at the level of its individual bits. It is a fast and simple action, basic to the higher-level arithmetic operations and directly supported by the processor.
INPUT:
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
docs=['why hello hello there','omg hello pony','she went there?omg']
print(docs)
vec=CountVectorizer()
x=vec.fit_transform(docs)
print('vocabulary',vec.vocabulary_)
print(x)
df=pd.DataFrame(x.toarray(),columns=vec.get_feature_names())
print(df)
w1=input("enter word1: ")
w2=input("enter word1: ")
op=input("enter operator: ")
x=[]
for i in range(df.shape[0]):
    if(op=="&"):
        a=(list(df.loc[:,w1]))[i]&(list(df.loc[:,w2]))[i]
        x.append(a)
    if(op=="|"):
        a=(list(df.loc[:,w1]))[i]|(list(df.loc[:,w2]))[i]
        x.append(a)
print(x)
for i in range(df.shape[0]):
    if(x[i]==1):
        print("Doc",i)
OUTPUT:
 


















Practical 2
AIM:  Implement Page Rank Algorithm.
WRITE UP:
PageRank is an algorithm used by Google Search to rank web pages in their search engine results. It is named after both the term "web page" and co-founder Larry Page. PageRank is a way of measuring the importance of website pages
INPUT:
import numpy as np
from fractions import Fraction
def display_format (my_vector, my_decimal):
    return np.round ( (my_vector).astype (float), decimals=my_decimal)
dp= Fraction (1,3)
M= np.matrix ([[0, Fraction (1,2), Fraction(1,2)],
[1, 0, 0],
[1, 0, 0]]
)
print (M)
E = np.zeros((3,3))
E[:] = dp
beta=0.9
A = beta * M + ((1-beta)* E)
r = np.matrix ([dp, dp, dp])
r = np. transpose (r)
previous_r= r
for i in range (1,10):
    r = A * r
    print (r)
    print (display_format(r, 3))
    if (previous_r==r).all():
        break
previous_r = r
print ("Final: \n", display_format(r, 3))
print ("sum", np.sum (r))
OUTPUT:






  





Practical 3
AIM: Implement Dynamic programming algorithm for computing the edit distance between
strings s1 and s2. (Hint. Levenshtein Distance)
WRITE UP:
In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits required to change one word into the other.
INPUT:
def editDistance(str1,str2,m,n):
    if m==0:
        return 0
    if n==0:
        return m
    if(str1[m-1]==str2[n-1]):
        return editDistance(str1,str2,m-1,n-1) 
    return 1+ min(editDistance(str1,str2,m,n-1),#insert
                  editDistance(str1,str2,m-1,n),#remove
                  editDistance(str1,str2,m-1,n-1))  #replace
str1="saturday"
str2="sunday"
print("the edit distance is:",editDistance(str1,str2,len(str1),len(str2))
      , "between",str1, "and",str2)
OUTPUT:
 













Practical 4
AIM: Write a program to Compute Similarity between two text documents
WRITE UP:
The traditional approach to compute text similarity between documents is to do so by transforming the input documents into real-valued vectors. The goal is to have a vector space where similar documents are “close”, according to a chosen similarity measure.

This approach takes the name of Vector Space Model, and it’s very convenient because it allows us to use simple linear algebra to compute similarities. We just have to define two things:
A way of transforming documents into vectors
A similarity measure for vectors
The simplest way to build a vector from text is to use word counts.

File1.txt: Information retrieval in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those   resources.

File2.txt: Information retrieval in computing and information science is the process of obtaining information system resources that relevant to an information need from  of those resources

INPUT:

import math
import string
import sys

def read_file(filename):
    try:
        with open(filename,'r') as f:
            data=f.read()
        return data
    except IOError:
        print("Error opening file",filename)
        sys.exit()

translation_table=str.maketrans(string.punctuation+string.ascii_uppercase," "*len(string.punctuation)+string.ascii_lowercase)

def get_words_from_line_list(text):
    text=text.translate(translation_table)
    word_list=text.split()
    return word_list

def count_frequency(word_list):
    D={}
    for new_word in word_list:
        if new_word in D:
            D[new_word]+D[new_word]+1
        else:
            D[new_word]=1
    return D

def word_frequency_for_file(filename):
    line_list=read_file(filename)
    word_list=get_words_from_line_list(line_list)
    freq_mapping=count_frequency(word_list)
    print("File",filename,":",)
    print(len(line_list),"letters,",)
    print(len(word_list),"words,",)
    print(len(freq_mapping),"distinct words")
    return freq_mapping
def dotProduct(D1,D2):
    sum=0.0
    for key in D1:
        if key in D2:
            sum+=(D1[key]*D2[key])
    return sum

def vector_angle(D1,D2):
    numerator=dotProduct(D1,D2)
    denominator=math.sqrt(dotProduct(D1,D1)*dotProduct(D2,D2))
    return math.acos(numerator/denominator)

def  documentSimilarity(filename_1,filename_2):
    sorted_word_list_1=word_frequency_for_file(filename_1)
    sorted_word_list_2=word_frequency_for_file(filename_2)
    distance=vector_angle(sorted_word_list_1,sorted_word_list_2)
    print("The distance between the documents is:%0.6f (radians)"%distance)
documentSimilarity("file1.txt","file2.txt")

OUTPUT:
 







Practical 5
AIM: Write a map-reduce program to count the number of occurrences of each alphabetic character in the given dataset. The count for each letter should be case-insensitive (i.e., include both upper-case and lower-case versions of the letter; Ignore non-alphabetic characters).
WRITE UP:
Our task is to count the frequency of each character present in our input file. We are using python for implementing this particular scenario. However, The MapReduce program can also be written in Java or C++. Execute the below steps to complete the task for finding the occurrence of each character.
INPUT:
from collections import Counter
import re
test = input("Enter a String: ")
string = re.sub("[^a-zA-Z]+", "", test)
print(string)
res = Counter(string.casefold())
print ("Output " + str(res))

OUTPUT:
 










Practical 6
AIM: Write a program for Pre-processing of a Text Document: stop word removal.
WRITE UP:
Stop words are any word in a stop list which are filtered out before or after processing of natural language data. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list.
INPUT:
import nltk
nltk.download ('stopwords')
nltk.download ('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
file=open(r"C:\Users\nikhi\OneDrive\Documents\sem 6 practicals\Ir\message.txt",'r')
sent=file.read()
stop=set (stopwords.words ('english'))
token=word_tokenize (sent)
a=[]
for w in token:
    if w not in stop:
           a.append (w)
print("Original sentence : ", token)
print("="*45)
print ("Stop words: ",stop)
print("="*45)
print ("Stop words removal : ",a)
print(" ".join(a))

OUTPUT:
 








Practical 8
AIM: Write a program to implement simple web crawler.
WRITE UP:
A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing (web spidering).
INPUT:
import requests
from bs4 import BeautifulSoup
plain_text=requests.get('https://codewithnick.github.io/').text
s=BeautifulSoup(plain_text,"html.parser")
for link in s.findAll('img'):
    tet=link.getText()
    print(tet)
    print(link)
print(plain_text)
print("") 
print(s.text)
print(s.status_code)

OUTPUT:
 



















Practical 9
AIM: Write a program to parse XML text, generate Web graph and compute topic specific page rank.
WRITE UP:
XML: XML stands for eXtensible Markup Language. It was designed to store and transport data. It was designed to be both human- and machine-readable.That’s why, the design goals of XML emphasize simplicity, generality, and usability across the Internet.
The XML file to be parsed in this tutorial is actually a RSS feed.

RSS: RSS(Rich Site Summary, often called Really Simple Syndication) uses a family of standard web feed formats to publish frequently updated informationlike blog entries, news headlines, audio, video. RSS is XML formatted plain text.

The RSS format itself is relatively easy to read both by automated processes and by humans alike.
The RSS processed in this tutorial is the RSS feed of top news stories from a popular news website. You can check it out here. Our goal is to process this RSS feed (or XML file) and save it in some other format for future use.
INPUT:
#Python code to illustrate parsing of XML files
# importing the required modules
import csv
import requests
import xml.etree.ElementTree as ET

def loadRSS():
	# url of rss feed
	url = 'http://wwwnc.cdc.gov/eid/rss/ahead-of-print.xml'
	# creating HTTP response object from given url
	resp = requests.get(url)
	# saving the xml file
	with open('topnewsfeed.xml', 'wb') as f:
		f.write(resp.content)
		
def parseXML(xmlfile):
	# create element tree object
	tree = ET.parse(xmlfile)
	# get root element
	root = tree.getroot()
	# create empty list for news items
	newsitems = []
	# iterate news items
	for item in root.findall('./channel/item'):
		# empty news dictionary
		news = {}

		# iterate child elements of item
		for child in item:
			# special checking for namespace object content:media
			if child.tag == '{http://search.yahoo.com/mrss/}content':
				news['media'] = child.attrib['url']
			else:
				news[child.tag] = child.text.encode('utf8')
		# append news dictionary to news items list
		newsitems.append(news)
	# return news items list
	return newsitems


def savetoCSV(newsitems, filename):
	# specifying the fields for csv file
	fields = ['guid', 'title', 'pubDate', 'description', 'link', 'media']
	# writing to csv file
	with open(filename, 'w') as csvfile:
		# creating a csv dict writer object
		writer = csv.DictWriter(csvfile, fieldnames = fields)
		# writing headers (field names)
		writer.writeheader()
		# writing data rows
		writer.writerows(newsitems)


def main():
	# load rss from web to update existing xml file
	loadRSS()
	# parse xml file
	newsitems = parseXML('topnewsfeed.xml')
	# store news items in a csv file
	savetoCSV(newsitems, 'topnews.csv')	
if __name__ == "__main__":
	# calling main function
	main()
OUTPUT:

 

